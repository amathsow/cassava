{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, Dense, MaxPool2D, Dropout, Concatenate, Flatten, GlobalMaxPooling2D\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.densenet import DenseNet169\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EgoCNN:\n",
    "    \n",
    "    def __init__(self, in_size, out_size,\n",
    "                 batch_size=10,\n",
    "                 handle=None,\n",
    "                 xrange=(-3,4), yrange=(-3,4),\n",
    "                 trange=(-3,4), tbins=20):\n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "        self.batch_size = batch_size\n",
    "        self.handle = handle\n",
    "        self.xrange = xrange\n",
    "        self.yrange = yrange\n",
    "        self.trange = trange\n",
    "        self.tbins = tbins\n",
    "        self.model = self.ego_cnn()\n",
    "    \n",
    "    def bcnn(self):\n",
    "        '''\n",
    "        Siamese CNN which makes up the bottom part of the network together with its twin.\n",
    "        '''\n",
    "        input_tensor = Input(shape=self.in_size)\n",
    "        x = Conv2D(96, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu')(input_tensor)\n",
    "        x = MaxPool2D(pool_size=(3,3))(x)\n",
    "        x = Conv2D(256, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu')(x)\n",
    "        x = MaxPool2D(pool_size=(3,3))(x)\n",
    "        x = Conv2D(256, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu')(x)\n",
    "        x = MaxPool2D(pool_size=(3,3), name='bcnn_output')(x)\n",
    "        return Model(input_tensor, x)     \n",
    "    \n",
    "    def tcnn(self, bottom_shape=None):\n",
    "        '''\n",
    "        Fully connected layers to be plugged on top of the concat siamese CNNs.\n",
    "        bottom_shape: output shape of one of the bottom CNNs\n",
    "        y1_shape: number of classes/bins for lateral motion\n",
    "        y2_shape: number of classes/bins for rotation angles\n",
    "        '''\n",
    "        if bottom_shape==None:\n",
    "            raise Exception('bcnn_shape or output_shape not specified (None)')\n",
    "        input_streams = Input(shape=bottom_shape)\n",
    "        x = Dense(100, activation='relu')(input_streams)\n",
    "        x = Dropout(rate=.5)(x)\n",
    "        Y = Flatten()(x)\n",
    "        return Model(input_streams, Y)\n",
    "    \n",
    "    def ego_cnn(self):\n",
    "        '''\n",
    "        Keras model putting together bottom and top NN\n",
    "        in_size: input image shape, must be coherent with bcnn's input size\n",
    "        out_size: tuple with number of classes/bins for translations and rotations\n",
    "        '''\n",
    "        in_stream1 = Input(shape=self.in_size, name='input0')\n",
    "        in_stream2 = Input(shape=self.in_size, name='input1')\n",
    "        bottom = self.bcnn()# self.in_size\n",
    "        middle = Concatenate()([bottom(in_stream1), bottom(in_stream2)])\n",
    "        middle_size = tuple(a*b for a,b in zip(bottom.output_shape[1:],[1,1,2]))\n",
    "        top = self.tcnn(middle_size)\n",
    "        output = top(middle)\n",
    "        output = [Dense(size, activation='softmax', name=f'output{num}')(output)\n",
    "                  for num, size \n",
    "                  in enumerate(self.out_size)]\n",
    "        return Model([in_stream1, in_stream2], output)\n",
    "    \n",
    "    def batch_generator(self):\n",
    "        \"\"\"\n",
    "        Extract a random batch of images, apply transformations and return both, \n",
    "        together with the corresponding transformation parameters\n",
    "\n",
    "        batch_size: batch size\n",
    "        handle: hdf5 dataset\n",
    "        xrange: (min, max+1) range of x translations (ints)\n",
    "        yrange: (min, max+1) range of y translations (ints)\n",
    "        trange: (min, max) range of z rotations\n",
    "        tbins: number of bins in which categorize the z rotations\n",
    "        \"\"\"\n",
    "        # extract batch_size random indices from the dataset for SGD\n",
    "        total_size = self.handle.shape[0]\n",
    "        while True:\n",
    "            indices = np.random.permutation(range(total_size))[0:self.batch_size]\n",
    "            indices = np.sort(indices)\n",
    "            # define batch_size x and y translations as integers in xrange and yrange\n",
    "            x_trans = np.random.randint(*self.xrange, size=self.batch_size)\n",
    "            y_trans = np.random.randint(*self.yrange, size=self.batch_size)\n",
    "            # define batch_size rotations in trange\n",
    "            z_rotat = np.random.uniform(*self.trange, size=self.batch_size)\n",
    "            # group them in tbins\n",
    "            z_rotat = np.digitize(z_rotat, np.linspace(*self.trange, self.tbins+1, endpoint=True))\n",
    "            # group each x, y , z transformation in a list of batch_size dictionaries\n",
    "            trans_list = [{'theta': t,'lx': x,'ly': y} for t, x, y in zip(z_rotat, x_trans, y_trans)]\n",
    "            # apply transformations to a batch (hdf5 dataset accepts lists not np.arrays)\n",
    "            datagen = ImageDataGenerator()\n",
    "            Xbatch_trans = np.array([datagen.apply_transform(img,tran) \n",
    "                                     for img, tran \n",
    "                                     in zip(self.handle[list(indices)],trans_list)])\n",
    "            Xbatch = self.handle[list(indices)]\n",
    "            # turning classes into categorical values\n",
    "            #classes need to be positive\n",
    "            Ybatch_tx = to_categorical(x_trans-self.xrange[0], sum(np.absolute(self.xrange))) \n",
    "            #classes need to be positive\n",
    "            Ybatch_ty = to_categorical(y_trans-self.yrange[0], sum(np.absolute(self.yrange))) \n",
    "            Ybatch_rot = to_categorical(z_rotat-1, self.tbins) #classes need to start from 0\n",
    "            #yield (Xbatch, Xbatch_trans), (Ybatch_trans, Ybatch_rot)\n",
    "            yield ({'input0': Xbatch, 'input1': Xbatch_trans},\n",
    "                   {'output0': Ybatch_rot, 'output1': Ybatch_tx, 'output2': Ybatch_ty})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EgoDense(EgoCNN):\n",
    "    \"\"\"def bcnn(self):\n",
    "        model = DenseNet169(include_top=False,\n",
    "                            weights='imagenet',\n",
    "                            input_shape=self.in_size,\n",
    "                            pooling=None)\n",
    "        return model\"\"\"\n",
    "    \n",
    "    def tcnn(self, bottom_shape=None):\n",
    "        '''\n",
    "        Fully connected layers to be plugged on top of the concat siamese CNNs.\n",
    "        bottom_shape: output shape of one of the bottom CNNs\n",
    "        y1_shape: number of classes/bins for lateral motion\n",
    "        y2_shape: number of classes/bins for rotation angles\n",
    "        '''\n",
    "        if bottom_shape==None:\n",
    "            raise Exception('bcnn_shape or output_shape not specified (None)')\n",
    "        input_streams = Input(shape=bottom_shape)\n",
    "        x = GlobalMaxPooling2D()(input_streams)\n",
    "        x = Dense(200, activation='relu')(x)\n",
    "        Y = Dropout(rate=.5)(x)\n",
    "        #Y = Flatten()(x)\n",
    "        return Model(input_streams, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('extraimages_224.hdf5') \n",
    "ego_cnn = EgoDense(in_size=(224,224,3), out_size=(20,7,7), handle=f['train_img'])\n",
    "model = ego_cnn.model\n",
    "model.compile(optimizer='adam',\n",
    "              loss={'output0': 'categorical_crossentropy',\n",
    "                    'output1': 'categorical_crossentropy',\n",
    "                    'output2': 'categorical_crossentropy'},\n",
    "              metrics=['acc'])\n",
    "#train_generator = ego_cnn.batch_generator()\n",
    "#model.fit_generator(train_generator,\n",
    "#                    steps_per_epoch=1,\n",
    "#                    epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input0 (InputLayer)             (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input1 (InputLayer)             (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_17 (Model)                (None, 8, 8, 256)    814208      input0[0][0]                     \n",
      "                                                                 input1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 8, 8, 512)    0           model_17[1][0]                   \n",
      "                                                                 model_17[2][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "model_18 (Model)                (None, 200)          102600      concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "output0 (Dense)                 (None, 20)           4020        model_18[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "output1 (Dense)                 (None, 7)            1407        model_18[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "output2 (Dense)                 (None, 7)            1407        model_18[1][0]                   \n",
      "==================================================================================================\n",
      "Total params: 923,642\n",
      "Trainable params: 923,642\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ego_cnn.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cassava_env)",
   "language": "python",
   "name": "cassava_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
